@@ -1,91 +1,58 @@
# 09 â€” Infrastructure as Code (Terraform)

# ğŸš€ DataFlow Logistics â€” Cloudification Project
Infraestrutura do projeto provisionada com **Terraform**: Azure + Databricks + Data Lake + ADF + Key Vault.

Projeto completo de **migraÃ§Ã£o e modernizaÃ§Ã£o** de sistemas legados **onâ€‘premises** para uma arquitetura **cloud Lakehouse (Databricks + Azure)**. Este repositÃ³rio foi pensado como **portfÃ³lio de engenharia de dados** com cÃ³digo, diagramas, governanÃ§a e CI/CD.
## PrÃ©-requisitos
- **Terraform 1.6+**
- **Azure CLI** autenticado (`az login`)
- PermissÃµes na assinatura Azure para criar RG, Storage, KV, ADF e Databricks

> **Elevator pitch:** Migration and modernization of the organizationâ€™s legacy onâ€‘premises systems as part of a broader cloud adoption and digital transformation initiative.

## ğŸ—ï¸ Arquitetura Alvo (VisÃ£o Geral)

```mermaid
flowchart LR
    subgraph OnPrem[Onâ€‘Premises]
      ERP[(MySQL - ERP)]
      FIN[(DB2 - Financeiro)]
      FTP[FTP - Arquivos CSV]
    end

    ADF[Azure Data Factory / Workflows]
    ADLS[(Azure Data Lake Storage)]
    DBX[Databricks Lakehouse
Spark e Delta]
    UC[Unity Catalog]
    PBI[Power BI]

    ERP -->|JDBC| ADF
    FIN -->|JDBC| ADF
    FTP -->|Copy| ADF
    ADF --> ADLS
    ADLS --> DBX
    DBX --> UC
    DBX --> PBI
```

## ğŸ“‚ Estrutura do RepositÃ³rio
> AutenticaÃ§Ã£o: o provider `azurerm` usa a sessÃ£o do Azure CLI por padrÃ£o. O provider `databricks` Ã© configurado via `azure_workspace_resource_id`, usando sua identidade do Azure (sem precisar de PAT).

## Estrutura
```
01-architecture/
02-datalake-design/
03-ingestion/
04-transformations/
05-orchestration/
06-analytics/
07-data-governance/
08-ci-cd/
.github/workflows/
09-infrastructure/
â”œâ”€â”€ backend.tf                 # (Opcional) backend remoto em Azure Storage (comentado)
â”œâ”€â”€ providers.tf               # providers azurerm e databricks
â”œâ”€â”€ versions.tf                # versÃµes mÃ­nimas
â”œâ”€â”€ variables.tf               # variÃ¡veis de root
â”œâ”€â”€ main.tf                    # orquestra mÃ³dulos
â”œâ”€â”€ outputs.tf                 # saÃ­das Ãºteis
â”œâ”€â”€ env/
â”‚   â”œâ”€â”€ dev.tfvars            # var-file ambiente dev
â”‚   â””â”€â”€ prod.tfvars           # var-file ambiente prod
â””â”€â”€ modules/
    â”œâ”€â”€ azure/
    â”‚   â”œâ”€â”€ main.tf
    â”‚   â”œâ”€â”€ variables.tf
    â”‚   â””â”€â”€ outputs.tf
    â””â”€â”€ databricks/
        â”œâ”€â”€ main.tf
        â”œâ”€â”€ variables.tf
        â””â”€â”€ outputs.tf
```

## ğŸ§° Stack
- **Databricks (PySpark, Delta Lake)**
- **Azure Data Lake Storage**
- **Azure Data Factory / Databricks Workflows**
- **Power BI**
- **Great Expectations**, **Unity Catalog**
- **GitHub Actions** (CI/CD)

## â–¶ï¸ Como Executar Localmente (dev)
1. Crie ambiente:
   ```bash
   python -m venv .venv && source .venv/bin/activate
   pip install -r requirements.txt
   cp .env.example .env
   # Preencha as variÃ¡veis no .env (NÃƒO COMMITAR)
   ```
2. Rode testes e linters:
   ```bash
   ruff check .
   pytest -q
   ```
3. Em Databricks, use **secrets** no lugar do `.env` (ver `03-ingestion/ingestion-readme.md`).

## ğŸ“ PadrÃµes Lakehouse (Bronze / Silver / Gold)
As camadas e convenÃ§Ãµes estÃ£o em `02-datalake-design/`.
## Como usar (DEV)
```bash
cd 09-infrastructure
az login
az account set --subscription "<SUBSCRIPTION_ID>"

## ğŸ”’ SeguranÃ§a e GovernanÃ§a
- Unity Catalog para permissÃµes e catÃ¡logo de dados.
- Great Expectations para validaÃ§Ã£o de qualidade.
terraform init
terraform plan  -var-file=env/dev.tfvars
terraform apply -var-file=env/dev.tfvars
```

## ğŸ—ºï¸ Roadmap
- [x] Estrutura inicial do repositÃ³rio
- [x] .gitignore e LICENSE (MIT)
- [x] Diagramas (Mermaid)
- [x] Scripts de ingestÃ£o (MySQL, DB2, FTP)
- [x] TransformaÃ§Ãµes (Bronzeâ†’Silverâ†’Gold)
- [x] CI bÃ¡sico com GitHub Actions
- [ ] Exemplos Power BI (mock)
- [ ] Exemplo de ADF export
## Backend remoto (state) â€” opcional
Edite `backend.tf` e crie previamente um Storage Account/Container para `tfstate`. Depois rode:
```bash
terraform init -migrate-state
```

## ğŸ“„ LicenÃ§a
MIT â€” veja `LICENSE`.
## Dica
- O mÃ³dulo `azure` cria: Resource Group, ADLS Gen2 (containers bronze/silver/gold), Key Vault,
  Data Factory e **Azure Databricks Workspace**.
- O mÃ³dulo `databricks` conecta no workspace criado e (opcionalmente) cria recursos internos
  como um **cluster de engenharia** e um **SQL Warehouse** simples.

---
> Desative recursos Databricks se nÃ£o tiver permissÃµes suficientes definindo `enable_databricks_resources = false` no `*.tfvars`.
